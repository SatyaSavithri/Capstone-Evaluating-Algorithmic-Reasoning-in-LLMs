# hybrid_runner_eval.py
"""
Model wrapper used by evaluation_runner.py and other scripts.
This file extends your earlier wrapper and guarantees:
 - .generate(prompt, ...)
 - .generate_with_activations(prompt, ...)
 - .tokenizer attribute
Uses HF AutoModelForCausalLM where possible.
"""

import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

logger = logging.getLogger("hybrid_runner_eval")
logging.basicConfig(level=logging.INFO)


class TransformersLLM:
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model_name = model_name
        self.device = torch.device(device if torch.cuda.is_available() or device == "cpu" else "cpu")
        logger.info(f"Loading model {model_name} to device {self.device} ... (this may take a while)")

        # tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

        # model - load with sensible defaults
        # We use device_map="auto" if running in environment with accelerate; fallback to cpu
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True,
                output_attentions=True,
                output_hidden_states=True
            )
        except Exception as e:
            logger.warning(f"HF load with device_map failed ({e}), retrying with cpu...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map={"": "cpu"},
                torch_dtype=torch.float32,
                output_attentions=True,
                output_hidden_states=True
            )

        self.model.eval()
        # move to device if model not already sharded onto devices
        try:
            self.model.to(self.device)
        except Exception:
            pass

        logger.info(f"Model {model_name} loaded.")

    def generate(self, prompt: str, max_new_tokens: int = 128, temperature: float = 0.0):
        """
        Deterministic generation using the HF generate API. Returns full decoded string.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                temperature=temperature,
                pad_token_id=self.tokenizer.eos_token_id
            )
        return self.tokenizer.decode(out[0], skip_special_tokens=True)

    @torch.no_grad()
    def generate_with_activations(self, prompt: str, max_new_tokens: int = 64):
        """
        Perform a forward pass to obtain hidden states & attentions.
        WARNING: this is not autoregressive token-by-token capture; it's a single forward
        that returns hidden states for input tokens. Many HF models won't return activations
        for tokens generated by .generate(). For full token-level capture you need to
        implement incremental decode with use_cache=False and loop generation; we keep
        this method as a reliable fallback used in the capstone pipeline.
        Returns dict with keys: hidden_states (tuple), attentions (tuple), model_output (raw)
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        try:
            outputs = self.model(
                **inputs,
                output_attentions=True,
                output_hidden_states=True,
                return_dict=True,
                use_cache=False
            )
        except TypeError:
            # some models don't accept use_cache parameter in forward; ignore
            outputs = self.model(
                **inputs,
                output_attentions=True,
                output_hidden_states=True,
                return_dict=True
            )
        # Convert tensors to CPU to make downstream processing safe
        hidden_states = tuple(h.detach().cpu() for h in outputs.hidden_states) if outputs.hidden_states is not None else None
        attentions = tuple(a.detach().cpu() for a in outputs.attentions) if outputs.attentions is not None else None
        return {"hidden_states": hidden_states, "attentions": attentions, "model_output": outputs}

    @torch.no_grad()
    def generate_activations_stepwise(self, prompt: str, max_new_tokens: int = 32):
        """
        Optional: token-by-token generation that captures hidden states/attentions after each generated token.
        This is more expensive and may not work on all models. Not used by default in evaluation_runner,
        but provided for completeness if you want true dynamic capture.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_ids = inputs["input_ids"]
        all_hidden = []
        all_attn = []
        cur_input = input_ids
        for _ in range(max_new_tokens):
            outputs = self.model(
                input_ids=cur_input,
                output_attentions=True,
                output_hidden_states=True,
                return_dict=True,
                use_cache=False
            )
            # collect last token's hidden state (batch, seq, hidden)
            hs = outputs.hidden_states[-1].detach().cpu()
            att = tuple(a.detach().cpu() for a in outputs.attentions) if outputs.attentions is not None else None
            all_hidden.append(hs)
            all_attn.append(att)
            # greedy next token
            logits = outputs.logits[:, -1, :]
            next_tok = torch.argmax(logits, dim=-1).unsqueeze(-1)
            cur_input = torch.cat([cur_input, next_tok.to(cur_input.device)], dim=1)
        return {"hidden_seq": all_hidden, "attn_seq": all_attn}
